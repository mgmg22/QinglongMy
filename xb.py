#!/bin/env python3
# -*- coding: utf-8 -*-
"""
cron: 3/30 8/1 * * * xb.py
new Env('çº¿æŠ¥0818');
"""
from bs4 import BeautifulSoup
import requests
from date_utils import get_day_string
from sendNotify import is_product_env, dingding_bot_with_key, send_wx_push
import sqlite3
import re
import asyncio
from openai_utils import AIHelper
from dotenv import load_dotenv
import json

key_name = "xb"
xb_list = []


class DBHelper:
    def __init__(self, db_name):

        self.conn = sqlite3.connect(db_name)
        self.cursor = self.conn.cursor()
        self.cursor.execute('''
        CREATE TABLE IF NOT EXISTS titles (
            id INTEGER PRIMARY KEY,
            path INTEGER,
            name TEXT UNIQUE NOT NULL,
            href TEXT NOT NULL
        )
        ''')

    def insert_many(self, items):
        tuples_list = [(x['path'], x['title'], x['href']) for x in items]
        self.cursor.executemany('INSERT OR IGNORE INTO titles (path,name, href) VALUES (?, ?, ?)', tuples_list)
        self.conn.commit()

    def fetch_all(self):
        self.cursor.execute('SELECT * FROM titles')
        return self.cursor.fetchall()

    def close(self):
        if self.cursor:
            self.cursor.close()
        if self.conn:
            self.conn.close()

    conn = sqlite3.connect(f'{key_name}.db')
    cursor = conn.cursor()
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS titles (
        id INTEGER PRIMARY KEY,
        path INTEGER,
        name TEXT UNIQUE NOT NULL,
        href TEXT NOT NULL
    )
    ''')


db = DBHelper(f'{key_name}.db')
load_dotenv()

cxkWhiteList = ["ä¸­å›½é“¶è¡Œ", "ä¸­è¡Œ", "å†œä¸šé“¶è¡Œ", "å†œè¡Œ", "äº¤é€šé“¶è¡Œ", "äº¤è¡Œ", "æµ¦å‘", "é‚®å‚¨", "é‚®æ”¿", "å…‰å¤§", "å…´ä¸š",
                "å¹³å®‰", "æµ™å•†", "æ­å·é“¶è¡Œ", "åŒ—äº¬é“¶è¡Œ", "å®æ³¢é“¶è¡Œ"]


def has_white_bank_name(content):
    return any(sub in content for sub in cxkWhiteList)


whiteWordList = [word for item in [
    "äº‘é—ªä»˜ ysf xyk æ€§ç”¨å¡ è¿˜æ¬¾ å·¥å•†é“¶è¡Œ å·¥å•† å·¥è¡Œ å·¥é“¶ eç”Ÿæ´» å»ºè®¾é“¶è¡Œ å»ºè¡Œ å»ºè æ‹›å•†é“¶è¡Œ æ‹›è¡Œ æŒä¸Šç”Ÿæ´» ä½“éªŒé‡‘ ä¸­ä¿¡ åŠ¨å¡ç©ºé—´",
    "æ‰‹æ·˜ å¤©çŒ« çŒ«è¶… æ”¯ä»˜å® zfb è½¬è´¦ æŸä»˜å® å¾®ä¿¡ wx vx v.x å°ç¨‹åº ç«‹å‡é‡‘ ljj å…¬ä¼—å· åŸæ–‡ æ¨æ–‡ äº¬ä¸œ ç‹—ä¸œ jd äº¬è±† eå¡ ç¾å›¢ elm",
    # "æ·˜å® tb æŠ–éŸ³ dy é—²é±¼ åŒç¨‹ æºç¨‹ é€”ç‰› éœ¸ç‹èŒ¶å§¬",
    "æ°´ å¿…ä¸­ çº¢åŒ… è™¹åŒ… æŠ½å¥– ç§’åˆ° ä¿åº• æ¸¸æˆ ä¸‹è½½ è¯è´¹ ç§»åŠ¨ å’ŒåŒ… ç”µä¿¡ qå¸ æ‰£å¸ éº¦å½“åŠ³ è‚¯å¾·åŸº å¿…èƒœå®¢ æ˜Ÿå·´å…‹ ç‘å¹¸ æœ´æœ´ å–œèŒ¶ ç¤¼å“å¡ æ˜Ÿç¤¼å¡ æ·±åœ³é€š ç½‘ä¸Šå›½ç½‘",
    "å›½è¡¥",
] for word in item.split()]


def has_white_word(content):
    return any(sub in content for sub in whiteWordList)


def has_black_xyk_name(content):
    xykNameList = ["xing/ç”¨å¡", "ä¿¡ç”¨å¡", "xingç”¨å¡", "ä¿¡ç”¨k", "å¿ƒç”¨å¡", "æ€§ç”¨å¡", "xyk", "ä¿¡yå¡", "æ·±åœ³"]
    return any(sub in content for sub in xykNameList) and has_white_bank_name(content)


def get_complete_content(content):
    # print(content)
    # Replace <br/> tags with newline characters before getting text
    for br in content.find_all('br'):
        br.replace_with('\n')
    text = content.get_text()
    # Find all 'a' tags in the content
    a_tags = content.find_all('a')
    for a_tag in a_tags:
        href = a_tag.get('href')
        if href:
            # Get the original text that contains this link
            original_text = a_tag.get_text()
            if re.search(r'http[s]?://\S+', original_text):
                # Replace the original text with markdown format
                markdown_link = f'[{href}]({href})'
                text = text.replace(original_text, markdown_link)
    return text


commonBlackList = [word for item in [
    "å®šä½ éƒ¨åˆ† ä¸œåŒ— å¾½ é™æ·±åœ³ åŒ—äº¬ å¤©æ´¥ é‡åº† æ·±åœ³åœ°åŒº å±±ä¸œ ç¦å»º æ±Ÿè‹ äº‘å— æ±Ÿè¥¿ æ²³åŒ— å¹¿ä¸œ å‰æ— æ¹–åŒ— æ²³å— é™•è¥¿ æ¹–å— å››å· å®å¤ å¹¿è¥¿ è¾½å® ç”˜è‚ƒ å†…è’™å¤ é’æµ· è´µå· å±±è¥¿ æ–°ç–†",
    "å¦é—¨ å—äº¬ ä¸œè å¹¿å· å—æµ· è‹å· ä¸­å±± å¸¸å· é’å²› æˆéƒ½ æ­¦æ±‰ åˆè‚¥ æ­é˜³ æ— é”¡ æµå— å¤§è¿ çŸ³å®¶åº„ æ³‰å· ä¸¹ä¸œ èŒ‚å é•¿æ²™ æ³°å· éƒ‘å· æƒ å· å¨æµ· ç»å…´ å“ˆå°”æ»¨ è´µé˜³",
    # ----å¡----
    "ä¸‡äº‹è¾¾ visa è½¦ä¸» åˆ·å¡è¾¾æ ‡ åŒ—åˆ† é™†é‡‘æ‰€ ç¼¤çº·ç”Ÿæ´» æµ¦å¤§å–œå¥” é‚®å‚¨è”å ç¾å›¢è”å é—ªå…‰å¡ è”åå¡ é‚®æ”¿æ•°å¸ å†œè¡Œåˆ·å¡é‡‘ å†œè¡Œæ•°å¸ ä¼šå‘˜è¿”ç° å…´ä¸šç”Ÿæ´» å†œä¿¡ é¢œå¡",
    "äº¤é€šé“¶è¡Œæ•°å¸ äº¤è¡Œæ•°å¸ äº¤é€šæ•°å¸ å…‰å¤§éº¦å½“åŠ³ é˜³å…‰æƒ ç”Ÿæ´» å…‰å¤§çŸ³åŒ– å¹¿å‘ æ’ä¸° æ±‡ä¸° å†œå•† è‹é“¶ å–„è ç™¾ä¿¡ å®çŸ³å±± ä¼—é‚¦ å…´å†œé€š é»‘å¡",
    # ----æ— æ•ˆ----
    "ç‰¹é‚€ å—é‚€ ç˜¦è…° æ”¶è…° ä¸´æœŸ å€¼å¾—ä¹°",
    # ----è¶…é“¾æ¥----
    "vip.iqiyi.com å‡ºé—¨é æœ‹å‹ æ·±iå·¥ ä»Šå¤©å¤šå°‘ 55618 ã€888ã€‘",
    # ----é—®é¢˜----
    "ä¹ˆ é—®é¢˜ é—®é—® é—®ä¸‹ è°¢è°¢ è¯·é—® é—®ä¸€ä¸‹ åˆ«é—® è¯·æ•™ æ±‚ å’‹ æ€æ · å’¨è¯¢ èµæ•™ å•¥ æœ‰é—® è¡Œä¸ ä½•è§£ ä¸è¡Œ åŸå›  å¸®å¿™çœ‹ å“ªæ¥çš„ éƒ½å¤šå°‘ æ˜¯å¤šå°‘ æ˜¯ä¸æ˜¯ æœ‰è° å¤§ä½¬",
    "æœç†Ÿ æœ‰æœ æ²¹æœ å½¦ç¥– äº¦è² å¤šä¸ è°æœ‰ æœ‰æ²¡ å¦‚ä½• é¢„ç®— ä½ ä»¬éƒ½ å‡ å· åˆ°åº• å¤šå°‘å‡º å“¥å“¥ä»¬",
    # ----æ•°ç ----
    "åä¸º huawei è£è€€æ‰‹æœº mipay æœå­ è‹¹æœ iphone airpods pm äºšç‘Ÿ å¤§ç–† æ•°æ® k70",
    # ----ç”Ÿæ´»å±…å®¶----
    "å…§è¡£ æ‹–é‹ æ´æ´é‹ è´­ç‰©è¢‹ å¸ƒè¢‹ å©´ ç«¥è£… å°å­© å­©å­ è¾£å¦ˆ æ— çº¿ ç‰™è† æ‹‰æ‹‰è£¤ ä¼ é”… ç”µåŠ¨è½¦ ç›” æ¸…é£ çº¸ ç»´è¾¾ æ¯ æ¤… èŒ çŸ³å¤´ å¥¥å¦™ å®¶æ”¿ å†ˆæœ¬ å—æäºº å·¾ æŸ“ ä¸€æ¬¡æ€§ èšŠé¦™ å®¹å£° æ²æµ´ åŠ›å£«",
    # ----é£é™©----
    "é£é™© ç¾å…ƒ æé¢ ä¿é™© å¼€é€š å¢ƒå¤– ç§’æ‰¹ ä¸‹å¡ å¼€æˆ· è´· å¾ä¿¡ è´¹ç‡ pos äººè„¸ å®¡æ‰¹ é»„ç‰› å®¢æœ å¤©å¤©åŸºé‡‘ ä¸œæ–¹è´¢å¯Œ",
] for word in item.split()]
highBlackList = [word for item in [
    # ----ç©æ³•----
    "ã€é¡¶ã€‘ éœ€è¦é‚€è¯· åŠ©åŠ› äººå›¢ æ‹¼å›¢ è°ƒç ” ç”³è¯·x äº’åŠ© æ”’èƒ½é‡ ç»„é˜Ÿ ç»„å›¢ é¦–å• ç›²ç›’ æœˆé»‘é£é«˜ äº’æ¢ å…¥ä¼š ä¹°1é€1 ä¹°ä¸€é€ä¸€ è’¸è’¸æ—¥ä¸Š æ·˜å®ç§’æ€ å¿…å… è†¨èƒ€",
    # ----ç½‘è´­----
    "00g æ”¯ ä»¶ /è¢‹ /ç›’ /æ–¤ ç®± ç½ xl è´§ ç“¶ é™ 9.9 å¦‚æœ‰ æŠ˜åˆ åˆ°æ‰‹ ä¹°å®¶ å°æ³•åº­ å•å· é¢„å”® è¯æœ¯ æ‹†å• æŸ¥è¯¢ é«˜ä½£ æƒ³ä¹° å°¾æ¬¾ å°é»„é±¼ æ”¾é‡ dyå•†åŸ äºŒæ‰‹ å‡ºå”®",
    # ----å½¢å®¹è¯----
    "é™é‡ å¥åº· åœ°é“ è¿›å£ çœŸè¯š å‰å®³ æœ‰ç‚¹6 éªš ä¸è¦è„¸ è›‹ç–¼ å¥‡æ€ª å¤§äº‹ è°± æ¶å¿ƒ å¤ªä¹± å¤ªè´µ çœŸçš„ å¥½ç©",
    # "ç“¶ è¿” å‡‘",
    # ----è¯­æ°”ç¬¦å·----
    "å‘¢ å‘€! å§ï¼Ÿ å•¦ ï¼ äº†ã€‚ã€‚ äº†å§ äº†å•Š å•Š~ ã€‚ã€‚ã€‚",
    # ----æƒ…ç»ª----
    "å§æ§½ é€¼ ç‰›b è¿˜å¥½ æ ¹æœ¬ æœ‰ç‚¹ä¸œè¥¿ æ„Ÿè§‰ å±…ç„¶ æ„Ÿè°¢ å¿ƒæ€ æ— è¯­ æ¯›çº¿ æ€¨ çœŸæ˜¯ æ— è€» ä¾¿å®œå•Š éº»çƒ¦ ä¸å¦‚ä¸ ç‹—äº† å·®äº† ç»ˆäº å¤ªæ¬¡äº† ä¸æƒ³æ åªæœ‰ ä¸æœ è§è¿‡ ç«Ÿç„¶",
    "å¤ªéš¾ çœ‹åˆ° å‘è´¢ å’± å£æ„Ÿ å¦¥å¦¥ æ­»æ´» å°±è¿™ ç‹—å± æ„Ÿå— æ²¡æƒ³åˆ° å…„å¼Ÿä»¬ åæ‚” åæ§½",
    # ----è´Ÿé¢----
    "ä¸èƒ½ åˆ äº† ç»­è´¹ é™åˆ¶ä½¿ç”¨ å†»ç»“ è¢«ç›— å·®è¯„ ç›‘æ§ å¥—ç‰¢ çŒ«é¥¼ æ€€ç–‘ ä¸çŸ¥é“ é»‘å¿ƒ æ²¡æœ‰ æƒ³æ³• ç½‘å‹ å…¬å¸ æŒ¤ ä¸Šç§‘æŠ€ ä¸å‹å¥½ éª— è¿”ä¹° åä¹° å…³é—­ é—²è°ˆ æŠ•è¯‰ è™šå‡ è¿›ç¾¤",
    "è¢«å° ç¦ ä¿®å¤ ç›¾ ç  èµ” ç§’é€€ é™é¢ å§é‡Œ èµ– æ°´è´´ åäº† çœŸçš„å‡çš„ å¦¹ èŠ æƒ¨ èµ·è¯‰ ç²ª å¥³ç”Ÿ æ„Ÿå†’",
    # ----æ—¶æ•ˆ----
    "ä»¥å å³å°† è¿‡æœŸ é•¿æœŸå‡º é€¾æœŸ é˜²èº« å‰10 å‰å¹´ æ”¶åˆ°çŸ­ä¿¡ ä¸Šä¸ªæœˆ æ”¹è§„åˆ™ å‡ ç§’ å¿˜è®° æƒ³èµ· æ˜å¤© å˜äº† åˆ é™¤ æœ€è¿‘ ä¸Šæ¬¡ å¯æ˜¯ è€",
    # ----end----
    "é»„äº† æ²¡äº† ä¸‹çº¿ å‡‰äº† é¢†ä¸äº† é¢†å®Œ æˆ‘æ‰ æ²¡æŠ¢åˆ° æ²¡åˆ° æœªåˆ°è´¦ ä¸åˆ°è´¦ å‡‰å‡‰ æŠ¢ä¸åˆ° ä¸Šé™ é€€æ¬¾ ä¸ç»™ ç å• æŠ¢å…‰ ä¸å¯ç”¨ å®¡æ ¸ æ¡æ¼ é¢†ä¸åˆ°",
    "ç¿»è½¦ å¤±è´¥ å´©æºƒ å´©äº† é»‘å· å·é»‘ æ‹¦æˆª å¸è½½ ç«çˆ† é”€æˆ· åƒåœ¾ é£æ§ ä¸ç©äº† æ¥ç”µ",
    # ----è™šæ‹Ÿ----
    "é«˜å¾· å·¥è¡Œç  å§ç  è¿…é›· ç”µå­ä¹¦ è½¦é™© æ¨¡æ¿ etc",
    # ----ä¸ç¬¦åˆé¢„æœŸçš„è¯è¯­----
    "æ²¡æ°´ çš„æ°´ æ¼æ°´ çº¯æ°´ ç¢±æ°´ æ°´æœ æ°´é›¾ å¸æ°´ ç²¾èƒæ°´ ç²¾åæ°´ å‡€æ°´ è¡¥æ°´ èŠ±éœ²æ°´ çƒ­æ°´ ç»ç’ƒæ°´ å£æ°´ ç¼©æ°´ æ°´é¾™ æ°´æ¶¦ æ°´ç‰› æ°´æª é¦™æ°´ æ°´å£¶ æ°´èœœ æ²¥æ°´ æ°´ä¹³ å¸å¦†æ°´ é˜²æ°´ é¥®æ°´ æ°´æ³¡ æ°´æ„Ÿ æ°´é¥º æ°´æ¨ æ°´å™¨ æ°´å«",
    "ç­¾åˆ°çº¢åŒ… è¿”çº¢åŒ… è¿”è™¹åŒ… æ¸¸æˆç§æœ æ¸¸æˆè´¦å· æœ‹å‹åœˆ è½¬ç½‘ cm ml",
] for word in item.split()]
lowBlackList = [word for item in [
    "å¤šæ‹ åˆ¸åŒ… å…å• é¢„å”® è¯•ç”¨ ç‚¹ç§’æ€ ä»¥æ—§æ¢æ–° å°ç¨‹åºä¸‹å• ç›´æ’­é—´ä¸‹å• æŠ˜åˆ¸ æ´¥è´´",
    # ----é£Ÿå“----
    "ä¸‰åªæ¾é¼  ç™¾è‰å‘³ æµ·åº•æ è®¤å…»ä¸€å¤´ç‰› ç«é”… çƒ§çƒ¤ éº»è¾£çƒ« é¦‹ å¤ ç‚– ç«è…¿ çˆª è›‹ç™½ è±†æµ† ç»´ç”Ÿç´  éº¦ç‰‡ é£é¹¤ ç²¥ é¢ å°éº¦ ç±³çº¿ ç²‰ ç²® è£™å¸¦èœ è’œ é˜¿èƒ¶ å·§å…‹åŠ› ç³– è›‹æŒ",
    # ----ç”Ÿé²œ----
    "å·§ä¹å…¹ æ¢¦é¾™ å¯ç”Ÿé£Ÿ ç¾Šè‚‰ è™¾ ç²½ ç¬‹ å¤§é—¸èŸ¹ æµ·å‚ æ¦´è² æ¢¨ æŸ æª¬ é¦™è‡ é²œèŠ± è“ ç«ç‘° é…¸èœ é›ªç³• å†°æ·‡æ·‹ æŸ¿ è„æ©™ ç“œ",
    # ----é¥®æ–™----
    "é¥®æ–™ æœæ± ç™¾å²å±± å†œå¤« çŸ¿æ³‰æ°´ èŒ…å° é…’ çª– ç‰¹ä»‘è‹ æ¤°å­ èŒ¶å¶ è§‚éŸ³ å¥¶èŒ¶ coco å¥ˆé›ª èœœé›ª èŒ¶ç™¾é“ å¤èŒ— åº“è¿ª å£æœ",
    # ----ç¾å¦†ä¸ªæŠ¤----
    "ç¾å¦† ç€è±é›… é›…è¯—å…°é»› æ¯›æˆˆå¹³ æ½˜å©· å±ˆè‡£æ° å¤§å® é‡‘çºº ç«‹ç™½ ç§‘é¢œæ° æ—æ¸…è½© æ¬§è±é›… è‹è² è”» æ´— çœ‰ å”‡ æ³¥ æ—¥æŠ› æŠ¤ç† é‡‡é”€ å«ç”Ÿ æ•æ„Ÿè‚Œ ä¿æ¹¿ ç¾ç™½ é˜²æ™’ ç²¾ç²¹ æµ·è“ å£ç½© olay è–‡ ç¾ç³",
    # ----å…¶ä»–å®ç‰©----
    "ä½› æœºæ²¹ å® ç‰© ç¿¡ç¿  è½®èƒ å›¾ä¹¦ å°å®¶ç”µ",
    # ----å“ç‰Œ----
    "ç¬¬ä¸‰æ–¹ äº¬é€  äº¬ä¸œä¹°è¯ ä¸¥é€‰ å–µæ»¡åˆ† æä½³ç¦ å·¥å‚ å®‡è¾‰",
    # ----è™šæ‹Ÿå¡åˆ¸----
    "ç«è½¦ ç”µå½± é—¨ç¥¨ æ‰“è½¦ é¡ºé£è½¦ å…»è½¦ æµé‡ gb å‡ºè¡Œä¼˜æƒ åˆ¸ ç½‘ç›˜ åœ°é“ ç½‘æ˜“äº‘ æœºç¥¨ åˆ«å¢… é¡ºä¸° å¿«é€’ å……ç”µ æ°‘å®¿ èŠ’æœ å¹´å¡ è…¾è®¯è§†é¢‘ ä½“æ£€",
    # ----çº¿ä¸‹é—¨åº—----
    "æ²ªä¸Šé˜¿å§¨ æ°¸å’Œå¤§ç‹ æ²ƒå°”ç› æ°¸è¾‰ ç›’é©¬ è”å costa æ¡Œæ¸¸ ç±³å…¶æ— å¥¥ç‰¹è±æ–¯ è¯•é©¾",
    # ----æ— æ•ˆ----
    "plus yzf ç¿¼æ”¯ä»˜ svip è”é€š ç§»åŠ¨å¥—é¤ ç¾å›¢åœˆåœˆ ç‹å¡ é’»çŸ³ä¼šå‘˜ é“‚é‡‘ é»‘é‡‘ è…¾è®¯vip èšæƒ å‡ºè¡Œ å‡¡ç§‘",
] for word in item.split()]


def filter_list(tr):
    title = tr.get_text().lower().strip()
    if title.endswith("ï¼Ÿ" or "?"):
        return False
    if tr['href'].startswith("http"):
        return False
    if "618.html" in tr['href']:
        return False
    href = 'http://www.0818tuan.com' + tr['href']
    match = re.search(r'/xbhd/(\d+)\.html', href)
    path_id = int(match.group(1))
    if has_black_xyk_name(title):
        print("----æ— è¯¥è¡Œä¿¡ç”¨å¡ï¼Œå·²å¿½ç•¥" + '\t\t' + href)
        return False
    all_blacklist = set(commonBlackList) | set(highBlackList) | set(lowBlackList)
    if any(sub in title for sub in all_blacklist):
        return False
    if not has_white_word(title) and not has_white_bank_name(title):
        return False
    for row in db.fetch_all():
        if path_id == row[1]:
            print('é‡å¤å·²å¿½ç•¥')
            return False
    content = get_content(href)
    # print(content)
    for checkItem in commonBlackList:
        if content and checkItem in content.get_text():
            print(f"{checkItem}\t\t----å…³é”®å­—ä¸åˆæ³•ï¼Œå·²å¿½ç•¥\t\t{href}")
            return False
    text = get_complete_content(content).strip()
    img_tags = content.find_all('img')
    src_list = []
    for img in img_tags:
        img_url = img.get('src')
        src_list.append(img_url)
    print(title + '\t\t' + href)
    item = {
        'title': title,
        'path': path_id,
        'href': href,
        'text': text,
        'src_list': src_list,
        'score': '',
    }
    xb_list.append(item)


def get_content(href):
    data = requests.get(href, proxies={})
    data.encoding = 'utf-8'
    soup = BeautifulSoup(data.text, 'html.parser')
    xb_content = soup.find('div', id='xbcontent')
    if not xb_content:
        print("è·å–ä¸åˆ°å¸–å­å†…å®¹")
        return ''
    first_p = xb_content.find('p')
    # print(first_p)
    if first_p:
        return first_p
    return xb_content


def get_top_summary():
    url = 'http://www.0818tuan.com/list-1-0.html'
    data = requests.get(url)
    data.encoding = 'utf-8'
    soup = BeautifulSoup(data.text, 'html.parser')
    tr_elements = soup.select('#redtag>.list-group-item')
    for tr in tr_elements:
        filter_list(tr)


def notify_markdown():
    if xb_list:
        if is_product_env():
            db.insert_many(xb_list)
        helper = AIHelper()
        prompt = f'''è¯·åˆ†æä»¥ä¸‹å†…å®¹çš„ä»·å€¼ï¼Œå¹¶è¿”å›ç¬¦åˆé¢„æœŸçš„å†…å®¹ã€‚

è¾“å‡ºè¦æ±‚ï¼š
1. å¿…é¡»è¿”å›æ ‡å‡†çš„ JSON æ•°ç»„
2. ä¸è¦è¿”å›ä»»ä½•å…¶ä»–å†…å®¹ï¼ˆå¦‚ ```json æ ‡è®°ï¼‰
3. æ¯ä¸ªå¯¹è±¡å¿…é¡»åŒ…å«ä»¥ä¸‹å­—æ®µï¼štitle, href, src_list, text, score
4. score å­—æ®µæ ¼å¼ä¸ºï¼šã€Œè¯„åˆ†1-5åˆ†ã€ä¸€å¥è¯ç®€è¦æ€»ç»“çš„ç†ç”±

ç¤ºä¾‹è¾“å‡ºæ ¼å¼ï¼š
[
    {{
        "title": "ç¤ºä¾‹æ ‡é¢˜",
        "href": "ç¤ºä¾‹é“¾æ¥",
        "src_list": ["å›¾ç‰‡é“¾æ¥1", "å›¾ç‰‡é“¾æ¥2"],
        "text": "ç¤ºä¾‹æ–‡æœ¬å†…å®¹",
        "score": "4åˆ†ã€ä¼˜æƒ åŠ›åº¦å¤§ï¼Œæ´»åŠ¨ç®€å•"
    }}
]

ç­›é€‰è§„åˆ™ï¼š
ç¬¦åˆé¢„æœŸçš„å†…å®¹åŒ…æ‹¬ï¼š
1. ç¾Šæ¯›æ´»åŠ¨
2. ä¼˜æƒ æ´»åŠ¨
3. çº¢åŒ…æˆ–ç°é‡‘æ´»åŠ¨
4. è™šæ‹Ÿå¡åˆ¸
5. ä¾¿å®œå•†å“
6. äº¬ä¸œæ´»åŠ¨
7. è¿™äº›é“¶è¡Œçš„æ— åœ°åŒºé™åˆ¶æ´»åŠ¨(
ä»…é™å€Ÿè®°å¡["ä¸­å›½é“¶è¡Œ","ä¸­è¡Œ","å†œä¸šé“¶è¡Œ","äº¤é€šé“¶è¡Œ","æµ¦å‘","é‚®å‚¨","é‚®æ”¿","å…‰å¤§","å…´ä¸š","å¹³å®‰","æµ™å•†","æ­å·é“¶è¡Œ","åŒ—äº¬é“¶è¡Œ", "å®æ³¢é“¶è¡Œ"],
å€Ÿè®°å¡å’Œä¿¡ç”¨å¡[å·¥å•†é“¶è¡Œ å·¥è¡Œ å·¥é“¶ eç”Ÿæ´» å»ºè®¾é“¶è¡Œ å»ºè æ‹›å•†é“¶è¡Œ æŒä¸Šç”Ÿæ´» ä¸­ä¿¡])
8. å¸¦æœ‰äºŒç»´ç çš„å›¾ç‰‡
9. æµ™æ±Ÿåœ°åŒºçš„æ´»åŠ¨

ä¸ç¬¦åˆé¢„æœŸçš„å†…å®¹åŒ…æ‹¬ï¼š
1. é—²èŠ,æ°´è´´
2. åæ§½
3. ç¾å¦†ä¸ªæŠ¤å•†å“
4. å¥³è£…å•†å“
5. é™å®šè¿™äº›åœ°åŒºçš„æ´»åŠ¨ï¼ˆæ·±åœ³ã€åŒ—äº¬ã€å¤©æ´¥ã€é‡åº†ï¼‰æ³¨æ„"ä¸Šæµ·äº¤é€šå¡"æ˜¯å…¨å›½é€šç”¨çš„æ´»åŠ¨
6. æé—®æˆ–æ±‚åŠ©å¸–(ä¹°ä»€ä¹ˆ ä¹°é‚£ä¸ª)
7. éƒ¨åˆ†é“¶è¡Œä¿¡ç”¨å¡æ´»åŠ¨["ä¸­å›½é“¶è¡Œ","å†œä¸šé“¶è¡Œ","äº¤é€šé“¶è¡Œ","æµ¦å‘", "é‚®å‚¨", "é‚®æ”¿", "å…‰å¤§", "å…´ä¸š","å¹³å®‰", "æµ™å•†","æ­å·é“¶è¡Œ","åŒ—äº¬é“¶è¡Œ","å®æ³¢é“¶è¡Œ"]
8. è¿™äº›é“¶è¡Œçš„æ•°å­—äººæ°‘å¸æ´»åŠ¨("å†œä¸šé“¶è¡Œ","äº¤é€šé“¶è¡Œ","æµ¦å‘","é‚®å‚¨","é‚®æ”¿","å…‰å¤§","å…´ä¸š","å¹³å®‰","æµ™å•†","æ­å·é“¶è¡Œ","åŒ—äº¬é“¶è¡Œ", "å®æ³¢é“¶è¡Œ" å·¥å•†é“¶è¡Œ æ‹›å•†é“¶è¡Œ ä¸­ä¿¡)

å¤„ç†è¦æ±‚ï¼š
1. ä¸ç¬¦åˆé¢„æœŸçš„å†…å®¹ä¸è¦è¿”å›
2. è‡ªåŠ¨çº æ­£titleä¸­çš„é”™åˆ«å­—æˆ–å­—æ¯ç¼©å†™ï¼ˆå¦‚ zfb->æ”¯ä»˜å®, vx->å¾®ä¿¡, dy->æŠ–éŸ³ï¼‰
3. ä¿æŒåŸæœ‰æ•°æ®ç»“æ„ï¼Œä»…åœ¨ score å­—æ®µä¸­æ·»åŠ è¯„åˆ†å’Œç†ç”±

å¾…åˆ†æå†…å®¹ï¼š
{xb_list}'''

        json_response = asyncio.run(helper.analyze_content(xb_list, prompt))
        json_data = json.loads(json_response)

        markdown_text = ''
        for item in json_data:
            markdown_text += f'''
##### ğŸ“Œ[{item['title']}ğŸŒŸ{item['score']}]({item['href']})
{item['text']}
'''
            for img in item['src_list']:
                markdown_text += f'![]({img})'
        summary = json_data[0]['title']
        # å‘é€é€šçŸ¥
        markdown_text += send_wx_push(summary, markdown_text, 37188)
        dingding_bot_with_key(summary, markdown_text, f"{key_name.upper()}_BOT_TOKEN")
        if is_product_env():
            dingding_bot_with_key(summary, markdown_text, "FLN_BOT_TOKEN")
        else:
            md_name = f"log_{key_name}_{get_day_string()}.md"
            with open(md_name, 'a', encoding='utf-8') as f:
                f.write("\n============================å¤„ç†åæ•°æ®===========================================\n")
                f.write(markdown_text)
    else:
        print("æš‚æ— çº¿æŠ¥ï¼ï¼")


def print_db():
    for row in db.fetch_all():
        print(row)


if __name__ == '__main__':
    try:
        # print_db()
        get_top_summary()
        notify_markdown()
    finally:
        db.close()
